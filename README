```
$ cargo run
$ cargo run benchmark
$ cargo run check


$ export LIBTORCH_USE_PYTORCH=1
$ cargo run solver
$ cargo run train
```

# Done

-   Simple MCTS solver (without transpositions), choosing actions for a single
    arm at a time in order
-   ResNet network architecture and basic training loop, with win-probability
    and policy heads
    -   Input features use sparse tensors for evaluation and training, to save
        bandwidth/space
-   Seed-solver that plays games starting from a human "seed solution",
    following human moves until a few moves from the end, and then uses the MCTS
    solver to play out the rest
-   Training data generator from partially-seeded games, generating two types of 
    input: human and MCTS
    -    Initially I used human inputs to train policy and MCTS inputs to train
         win-rate. This did okay but ran into two problems: not enough diverse
         training data and overfitting to blind repetitions of arm patterns
         ("fard", "faar", see observations below)

# In progress

-   Generate synthetic random problems by iterating backwards
-   Probe/visualise the feature inputs and network to make sure everything's
    working as I expected

# Observations

-   When training on samples derived from MCTS where we start the search from
    a-few-moves-from-the-end, the network very quickly overspecializes to spam
    "Drop"
    -   This was partially just because I forgot to clear the input between
        timesteps, meaning it was completely useless, which meant the best the
        network could do is to blindly spam the most likely instruction
    -   In response I also tweaked the loss-weight for samples that come from
        human solutions (which ~only sees policy loss) vs those that come from
        MCTS (which ~only sees value loss)
-   Training both value and policy on a mix of human solutions (weight) +
    weak MCTS solutions, I attempted to weight the loss scale to significantly
    (x0.00001) shrink the impact of value loss for human solutions. This has
    caused the value head to become extremely sharp for MCTS solutions, even
    though the contribution of the value loss to the overall loss is very
    small (0.006 vs 2.0).
    -   Actually, why _is_ the value loss so low here? Is it just predicting
        "cycles left until cycle limit > some small number"? I think so...
    -   Solution: Generate solutions that have higher cycles-left but still
        succeed, and lower cycles-left but still fail
-   Starting to see forms of overfitting, from training on limited numbers of
    human solutions.  In a particular solution to OM2021_W0, an arm has learnt
    to "fard" repeatedly, and another "faar", all the way to the end, even
    though this isn't actually the shortest/most optimal sequence -- it's just
    learned behaviour from the fact that this instruction-count solution repeats
    those motifs a lot.
    -   Solution: taper off the human training and generate more MCTS data?

# Future work

Technical/impl work:

-   Seed-solver: Generate solutions that have low cycles-left but still fail
    -   ...because the layout is bad
    -   ...because the layout is good but there's just not enough cycles left
-   Seed-solver: Randomly generate some training samples that have no history
-   Seed-solver: Evaluate and monitor
    -   proportion of successful solves (where it had to find _n_ moves,
        as a function of _n_?)
    -   how well does it solve with no history?
-   Train a timesteps-left head on existing solutions
-   Search: Parallelise MCTS, batch NN evaluations
-   Make net/eval/training board size agnostic
-   use symmetries to generate more data
-   Network/code for generating and estimating winrate for layouts
-   Solvers: Better logging, saving intermediate layer outputs / residuals
-   Policy network visualisation

Science-y investigation work:

-   Lean on existing solutions more, but be careful mixing them in with
    MCTS-generated solutions. They are from different domains; try having them
    train on non-overlapping outputs, to avoid the model overspecializing on
    "source"
-   weight and neuron activation visualization
    -   mean/stdev/activation-proportion per layer / per history
    -   max abs gradient
-   find/monitor dead neurons
-   keep more notes/observations for experiments, be more scientific about this
-   run same NN several times to get an idea of consistency
-   validation set in addition to training set
-   Batch size? https://arxiv.org/pdf/1812.06162

# idk

-   Does allowing action selection for any arm (rather than the next one in order)
    improve strength?
