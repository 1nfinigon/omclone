```
$ cargo run
$ cargo run benchmark
$ cargo run check


$ export LIBTORCH_USE_PYTORCH=1
$ cargo run solver
$ cargo run train
```

# Done

-   Simple MCTS solver (without transpositions), choosing actions for a single
    arm at a time in order
-   ResNet network architecture and basic training loop, with win-probability
    and policy heads
    -   Input features use sparse tensors for evaluation and training, to save
        bandwidth/space
-   Seed-solver that plays games starting from a human "seed solution",
    following human moves until a few moves from the end, and then uses the MCTS
    solver to play out the rest
-   Training data generator from partially-seeded games, generating two types of 
    input: human and MCTS
    -    Initially I used human inputs to train policy and MCTS inputs to train
         win-rate. This did okay but ran into two problems: not enough diverse
         training data and overfitting to blind repetitions of arm patterns
         ("fard", "faar", see observations below)

# In progress

-   Generate synthetic random problems by iterating backwards
-   Probe/visualise the feature inputs and network to make sure everything's
    working as I expected

# Experiment 0 (2024-09-27): just playing around

-   Training data: epoch-v1-n seeded ~100-1000hum:1mcts, later w/ split loss
    weight
-   Optimizer: AdamW lr between 4e-5 and 3e-3
-   Batch size: 128
-   Model: model v1 7 layer ch=96 (~2000k params)

Running the seed solver training on samples derived from MCTS where we start the
search from only a small number (x) of moves from the end, then training a new
network on the result; rinse and repeat. Increasing x gradually as the network
gets better. No systematic analysis or probing, just manually black-box
inspecting solver output and generated solutions to gain a bit of intuition and
spot obvious issues.

Notes:
    
-   The network very quickly overspecializes to spam "Drop"
    -   This was just because I forgot to clear the input between timesteps,
        meaning the input contained very little usable data, which meant the
        best the network could do is to blindly spam the most likely instruction
    -   In response to this observation I also tweaked the loss-weight for
        samples that come from human solutions (which ~only sees policy loss) vs
        those that come from MCTS (which ~only sees value loss)
-   Training both value and policy on a mix of human solutions (weight) +
    weak seeded MCTS solutions, I attempted to weight the loss scale to
    significantly (x0.00001) shrink the impact of value loss for human
    solutions. This has caused the value head to become extremely sharp (always
    outputting 0 or 1, no in-between) for MCTS solutions, even though the
    contribution of the value loss to the overall loss is very small (0.006 vs
    2.0).
    -   Actually, why _is_ the value loss so low here? Is it just predicting
        "cycles left until cycle limit > some small number"? I think so...
    -   Proposed solution: Generate solutions that have higher cycles-left but
        still succeed, and lower cycles-left but still fail
    -   After I increased the cycle-window for seeded solutions, and started
        generating solutions that have higher cycles-left but still succeed, the
        value loss was still extremely small. Why??
    -   It's because I have 100x the number of human-solution samples than MCTS
        samples (for a given seed game, the ratio is currently approx 1k-2k
        human samples vs 10 MCTS samples)
    -   I could compensate for this by increasing the importance weighting by a
        factor of ~100x, but that effectively 100x's my learning rate (TODO:
        apparently this is only true for SGD optimizer, but not quite true for
        AdamW optimizer? Figure out why), which my previous rudimentary
        experiments around lr has already shown to completely throw off the
        training stability.
    -   So, I just need to generate more, and more diverse data...
        -   Right now I have 1 GPU for training and 1 GPU for game-playing; it's
            looking like I want a ratio of 1:10 instead. Which is also approx
            the ratio the KataGo paper used, interestingly.
        -   As for more diverse data... working on generating synthetic random
            problems by iterating backwards from the final state.
-   Starting to see forms of overfitting, from training on limited numbers of
    human solutions.  In a particular solution to OM2021_W0, an arm has learnt
    to "fard" repeatedly, and another "faar", all the way to the end, even
    though this isn't actually the shortest/most optimal sequence -- it's just
    learned behaviour from the fact that this instruction-count solution repeats
    those motifs a lot.
    -   Proposed solution: taper off the human training and generate more MCTS
        data

# Experiment 1 (2024-09-28): effect of varying #channels

-   Training/validation data:
    -   epoch-v1-1 (seeded ~100hum:1mcts)
    -   w/ split loss weight (hum -> policy loss only, mcts -> value loss only)
    -   831k samples from 2.3k games
    -   75/25 training/validation split
-   Optimizer: AdamW lr=4e-4
-   Batch size: 128
-   Model A: model96_20240928_112658 v1 7 layer ch=96 (2048k params)
-   Model B: model48_20240928_112943 v1 7 layer ch=48 (764k params)

Notes:

-   This training data set still suffers from value loss being small
    relative to policy loss, this time it's more like a factor of 100x
-   L2 regularization penalty scales with # params! The current setup
    effectively means Model B is being regularised less than A (as the other
    loss terms do not scale). (And it also means that you can't directly
    compare the total loss numbers between different-sized networks.
    Although with the current L2 penalty coeff this is a negligible effect.)
-   Both networks train at the same speed, indicating the python dataloader or
    disk->sparse->dense->GPU or loss-computation overheads dominate. The GPU is
    not hitting power usage limits (avg 100W for A and 50W for B), further
    indicating that some time spent optimizing pytorch/train.py would be
    fruitful.
-   I neglected to set a seed, so the random training/validation split is
    different, making validation results hard to compare between the two. This
    is fixed for future experiments.

# Future work

Technical/impl work:

-   Seed-solver: Generate solutions that have low cycles-left but still fail
    -   ...because the layout is bad
    -   ...because the layout is good but there's just not enough cycles left
-   Seed-solver: Randomly generate some training samples that have no history
-   Seed-solver: Evaluate and monitor
    -   proportion of successful solves (where it had to find _n_ moves,
        as a function of _n_?)
    -   how well does it solve with no history?
-   Train a timesteps-left head on existing solutions
-   Search: Parallelise MCTS, batch NN evaluations
-   Make net/eval/training board size agnostic
-   use symmetries to generate more data
-   Network/code for generating and estimating winrate for layouts
-   Solvers: Better logging, saving intermediate layer outputs / residuals
-   Policy network visualisation

Science-y investigation work:

-   Lean on existing solutions more, but be careful mixing them in with
    MCTS-generated solutions. They are from different domains; try having them
    train on non-overlapping outputs, to avoid the model overspecializing on
    "source"
-   weight and neuron activation visualization
    -   mean/stdev/activation-proportion per layer / per history
    -   max abs gradient
-   find/monitor dead neurons
-   keep more notes/observations for experiments, be more scientific about this
-   run same NN several times to get an idea of consistency
-   Batch size? https://arxiv.org/pdf/1812.06162

# idk

-   Does allowing action selection for any arm (rather than the next one in order)
    improve strength?
